{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77f3681",
   "metadata": {
    "id": "c77f3681"
   },
   "source": [
    "# Import Necessary Libraries and configuration parameters\n",
    "\n",
    "This section imports all the necessary libraries required for the project. The libraries are organized by functionality, including asynchronous operations, deep learning, computer vision, API handling, and utility functions such as JSON and file management.\n",
    "\n",
    "- `asyncio`: For managing asynchronous operations in Python.\n",
    "\n",
    "- `os`: To interact with the operating system (e.g., file and directory management).\n",
    "\n",
    "- `fastapi.FastAPI`: For building APIs using FastAPI.\n",
    "- `fastapi.Path`: For handling API path parameters.\n",
    "- `fastapi.WebSocket`: For managing real-time WebSocket connections.\n",
    "- `fastapi.responses.HTMLResponse`: For sending HTML responses.\n",
    "\n",
    "- `json`: For working with JSON data (parsing and serializing).\n",
    "- `pickle`: For serializing and deserializing Python objects.\n",
    "- `pandas`: For data manipulation and analysis, particularly with tabular data.\n",
    "\n",
    "- `PIL.Image`: For handling image operations such as loading, saving, and manipulating images.\n",
    "- `cv2`: For computer vision tasks using OpenCV.\n",
    "- `torchvision.transforms`: For image transformations like resizing, cropping, and normalization.\n",
    "- `matplotlib.pyplot`: For visualizing data, including images and charts.\n",
    "\n",
    "- `torch`: For creating and working with deep learning models.\n",
    "- `torch.nn`: For defining neural networks and model components.\n",
    "- `transformers.DetrForObjectDetection`: For object detection tasks using Hugging Face's Detr model.\n",
    "- `transformers.GLPNForDepthEstimation`: For depth estimation using the GLPN model.\n",
    "- `transformers.GLPNFeatureExtractor`: For preprocessing images for the GLPN model.\n",
    "\n",
    "- `numpy`: For numerical operations, especially array and matrix manipulations.\n",
    "- `scipy.stats`: For statistical analysis and hypothesis testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "MXjHnSWT9SvJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXjHnSWT9SvJ",
    "outputId": "11adb8f6-42a0-467a-e4e6-d3cc12ff11b9",
    "ExecuteTime": {
     "end_time": "2025-03-21T15:29:25.999712Z",
     "start_time": "2025-03-21T15:29:07.972174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 1)) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 2)) (0.15.2+cu117)\n",
      "Requirement already satisfied: transformers>=4.35.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 3)) (4.50.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.7.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.3)\n",
      "Requirement already satisfied: opencv-python>=4.8.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 6)) (4.10.0.84)\n",
      "Requirement already satisfied: uvicorn>=0.24.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 7)) (0.31.0)\n",
      "Requirement already satisfied: fastapi>=0.104.1 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 8)) (0.115.0)\n",
      "Requirement already satisfied: gunicorn==21.2.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 9)) (21.2.0)\n",
      "Requirement already satisfied: setuptools>=68.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (68.0.0)\n",
      "Requirement already satisfied: scipy>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: websockets>=11.0.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 13)) (13.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 14)) (2.0.6)\n",
      "Requirement already satisfied: cython>=3.0.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 15)) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 16)) (1.26.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from gunicorn==21.2.0->-r requirements.txt (line 9)) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision>=0.15.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision>=0.15.0->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 5)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn>=0.24.0->-r requirements.txt (line 7)) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn>=0.24.0->-r requirements.txt (line 7)) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from fastapi>=0.104.1->-r requirements.txt (line 8)) (0.38.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi>=0.104.1->-r requirements.txt (line 8)) (1.10.8)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 12)) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn>=0.24.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\elfil\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.35.0->-r requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from starlette<0.39.0,>=0.37.2->fastapi>=0.104.1->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.15.0->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.15.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.15.0->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.15.0->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi>=0.104.1->-r requirements.txt (line 8)) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8565c486",
   "metadata": {
    "id": "8565c486",
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:28.487770Z",
     "start_time": "2025-03-21T15:32:25.907049Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.detr.modeling_detr because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'compiler'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1968\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1967\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1969\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1204\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\detr\\modeling_detr.py:27\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_outputs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     29\u001B[0m     ModelOutput,\n\u001B[0;32m     30\u001B[0m     add_start_docstrings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     35\u001B[0m     requires_backends,\n\u001B[0;32m     36\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:55\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mflash_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flash_attention_forward\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mflex_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flex_attention_forward\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdpa_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sdpa_attention_forward\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\integrations\\flex_attention.py:46\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattention\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mflex_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     42\u001B[0m         create_block_mask \u001B[38;5;28;01mas\u001B[39;00m create_block_causal_mask_flex,\n\u001B[0;32m     43\u001B[0m     )\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mWrappedFlexAttention\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03m    We are doing a singleton class so that flex attention is compiled once when it's first called.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\integrations\\flex_attention.py:61\u001B[0m, in \u001B[0;36mWrappedFlexAttention\u001B[1;34m()\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_instance\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mcompiler\u001B[38;5;241m.\u001B[39mdisable(recursive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;124;03m    Initialize or update the singleton instance.\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch' has no attribute 'compiler'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DetrForObjectDetection, GLPNForDepthEstimation, GLPNFeatureExtractor\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1229\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1957\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1955\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m   1956\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[1;32m-> 1957\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1958\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[0;32m   1959\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1956\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1954\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[0;32m   1955\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1956\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[0;32m   1957\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1958\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1970\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1969\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1970\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1971\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1972\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1973\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.models.detr.modeling_detr because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'compiler'"
     ]
    }
   ],
   "source": [
    "# Asynchronous operations\n",
    "'''import asyncio'''\n",
    "import os\n",
    "\n",
    "# Deep learning and computer vision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import DetrForObjectDetection, GLPNForDepthEstimation, GLPNFeatureExtractor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# API handling\n",
    "'''\n",
    "from fastapi import FastAPI, Path, WebSocket\n",
    "from fastapi.responses import HTMLResponse\n",
    "'''\n",
    "# Data handling and statistical analysis\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468b01e",
   "metadata": {
    "id": "8468b01e"
   },
   "source": [
    "The configuration parameters for model paths, device setup, and related resources. The configuration is designed to automatically handle device selection and streamline model loading paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3703e5",
   "metadata": {
    "id": "af3703e5",
    "ExecuteTime": {
     "end_time": "2025-03-21T15:29:27.160476Z",
     "start_time": "2025-03-21T15:29:27.160476Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'detr_model_path': 'facebook/detr-resnet-101',\n",
    "    'glpn_model_path': 'vinvino02/glpn-kitti',\n",
    "    'lstm_model_path': 'pretrained_lstm.pth',\n",
    "    'lstm_scaler_path': 'lstm_scaler.pkl',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a9e75",
   "metadata": {
    "id": "348a9e75"
   },
   "source": [
    "# Set Device for Model\n",
    "\n",
    "In this section, the device (CPU or GPU) is set for running the deep learning models, depending on the configuration provided in the `CONFIG` file. The goal is to ensure the model runs efficiently on available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d3a4a",
   "metadata": {
    "id": "879d3a4a",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.162473Z"
    }
   },
   "outputs": [],
   "source": [
    "device = CONFIG['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4606d3",
   "metadata": {
    "id": "cc4606d3"
   },
   "source": [
    "# Define the LSTM-based Z-location Estimator Model\n",
    "\n",
    "This section defines the architecture of the LSTM-based model used for predicting the Z-location (distance) of objects detected in the input images.\n",
    "The Zloc_Estimator is a neural network model designed for estimating Z-location (depth) based on a sequence of input features. It consists of an LSTM layer for handling sequential input, followed by fully connected layers for regression.\n",
    "\n",
    "### Parameters:\n",
    "- **input_dim** : *int*  \n",
    "    The dimensionality of the input features. Each time step of the input sequence has `input_dim` features.\n",
    "  \n",
    "- **hidden_dim** : *int*  \n",
    "    The number of hidden units in the LSTM layer. This determines the size of the LSTM's hidden state.\n",
    "  \n",
    "- **layer_dim** : *int*  \n",
    "    The number of LSTM layers stacked in the model. The model can have multiple layers of LSTM for increased capacity.\n",
    "\n",
    "### Model Architecture:\n",
    "1. **LSTM Layer**:  \n",
    "    - The model starts with an LSTM (Long Short-Term Memory) layer to process sequential input data.\n",
    "    - **Parameters**:\n",
    "        - `input_dim`: The input dimension of the LSTM layer.\n",
    "        - `hidden_dim`: The number of units in the LSTM's hidden state.\n",
    "        - `layer_dim`: The number of stacked LSTM layers.\n",
    "        - `batch_first=True`: Ensures that the input and output tensors are shaped as `(batch_size, sequence_length, feature_dim)`.\n",
    "        - `bidirectional=False`: The LSTM processes the sequence in only one direction.\n",
    "\n",
    "2. **Fully Connected Layers**:  \n",
    "    - After the LSTM, the model uses a sequence of fully connected (dense) layers to refine the output and produce the final prediction.\n",
    "    - The hidden dimension of the LSTM layer is progressively reduced in the fully connected layers.\n",
    "    - **Layer sizes**: [306, 154, 76].\n",
    "    - Each fully connected layer is followed by a **ReLU activation function** to introduce non-linearity.\n",
    "    - The final output layer has a **single neuron** (without activation) for regression.\n",
    "\n",
    "### Methods:\n",
    "- **`forward(x)`**  \n",
    "    Defines the forward pass of the model.\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **x** : *torch.Tensor*  \n",
    "            Input tensor of shape `(batch_size, sequence_length, input_dim)` representing the sequence of features for each batch.\n",
    "    \n",
    "    - **Returns**:\n",
    "        - **torch.Tensor**  \n",
    "            The final output of the model, representing the estimated Z-location for each input sequence.  \n",
    "            The output is a tensor of shape `(batch_size, 1)`, where each element is the predicted depth.\n",
    "\n",
    "### Forward Pass Process:\n",
    "1. **LSTM Output**:  \n",
    "    - The input `x` (a batch of sequences) is passed through the LSTM layer.\n",
    "    - The output of the LSTM is a tensor of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "2. **Fully Connected Layers**:  \n",
    "    - The model takes the output from the **last time step** of the LSTM (`out[:, -1]`), which captures the summary of the entire sequence.\n",
    "    - This output is passed through the fully connected layers to generate the final prediction.\n",
    "\n",
    "### Summary:\n",
    "- `__init__`: Initializes the model with the necessary input dimensions, hidden state dimensions, and the number of LSTM layers.\n",
    "- `forward`: Defines the forward pass logic, where the input features are processed through the LSTM layers and the output is the estimated Z-location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb95412",
   "metadata": {
    "id": "bdb95412"
   },
   "outputs": [],
   "source": [
    "class Zloc_Estimator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim):\n",
    "        super(Zloc_Estimator, self).__init__()\n",
    "\n",
    "        # LSTM layer\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Fully connected layers\n",
    "        layersize = [306, 154, 76]\n",
    "        layerlist = []\n",
    "        n_in = hidden_dim\n",
    "        for i in layersize:\n",
    "            layerlist.append(nn.Linear(n_in, i))\n",
    "            layerlist.append(nn.ReLU())\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layersize[-1], 1))  # Final output layer\n",
    "\n",
    "        self.fc = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hn = self.rnn(x)\n",
    "        output = self.fc(out[:, -1])  # Get the last output for prediction\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee633c4e",
   "metadata": {
    "id": "ee633c4e"
   },
   "source": [
    "# Deployment-ready Class for Handling the Model\n",
    "\n",
    "This section outlines the deployment-ready class for the LSTM-based Z-location estimator model. This class represents an LSTM (Long Short-Term Memory) model wrapper for Z-location estimation. It initializes the model with pre-defined architecture parameters and loads pre-trained weights.\n",
    "\n",
    "### Attributes:\n",
    "- **input_dim** : *int*  \n",
    "    The number of input features expected by the model. In this case, it's set to 15.\n",
    "- **hidden_dim** : *int*  \n",
    "    The number of units in the hidden layers of the LSTM model. Here, it's set to 612.\n",
    "- **layer_dim** : *int*  \n",
    "    The number of layers in the LSTM network. The model is designed with 3 layers.\n",
    "- **model** : *Zloc_Estimator*  \n",
    "    The LSTM model for estimating the Z-location (depth) based on the input features.  \n",
    "    It is initialized with the defined input, hidden dimensions, and number of layers.\n",
    "\n",
    "### Methods:\n",
    "- **`__init__(self)`**  \n",
    "    Constructor for the `LSTM_Model` class. It initializes the LSTM model with the given architecture,  \n",
    "    loads the model's pre-trained weights from a state dictionary, and moves the model to the correct  \n",
    "    device (CPU or GPU). Additionally, it sets the model to evaluation mode, making it ready for inference.\n",
    "  \n",
    "- **`predict(self, data)`**  \n",
    "    This method performs inference on the input data and returns the predicted Z-location (depth).\n",
    "    \n",
    "    - **Parameters:**\n",
    "        - **data** : *torch.Tensor*  \n",
    "            Input tensor of shape `(batch_size, input_dim)` representing the feature set for each batch.\n",
    "  \n",
    "    - **Returns:**\n",
    "        - **torch.Tensor**  \n",
    "            The predicted Z-location for each input sequence, returned as a tensor.\n",
    "\n",
    "### Summary:\n",
    "- **`__init__`**: Loads the pre-trained model and prepares it for deployment by setting the model to evaluation mode.\n",
    "- **`predict`**: Takes pre-processed input data and returns the predicted Z-location. The input is normalized before being passed through the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2bf21",
   "metadata": {
    "id": "fcf2bf21",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.168004Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input_dim = 15\n",
    "        self.hidden_dim = 612\n",
    "        self.layer_dim = 3\n",
    "\n",
    "        # Initialize the Z-location estimator model\n",
    "        self.model = Zloc_Estimator(self.input_dim, self.hidden_dim, self.layer_dim)\n",
    "\n",
    "        # Load the state dictionary from the file, using map_location in torch.load()\n",
    "        state_dict = torch.load(CONFIG['lstm_model_path'], map_location=device)\n",
    "\n",
    "        # Load the model with the state dictionary\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "        self.model.to(device)  # This line ensures the model is moved to the right device\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predicts the z-location based on input data.\n",
    "\n",
    "        :param data: Input tensor of shape (batch_size, input_dim)\n",
    "        :return: Predicted z-location as a tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Disable gradient computation for deployment\n",
    "            data = data.to(device)  # Move data to the appropriate device\n",
    "            data = data.reshape(-1, 1, self.input_dim)  # Reshape data to (batch_size, sequence_length, input_dim)\n",
    "            zloc = self.model(data)\n",
    "        return zloc.cpu()  # Return the output in CPU memory for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825c3ce",
   "metadata": {
    "id": "0825c3ce"
   },
   "source": [
    "# DETR (Detection Transformer) Class for Object Detection\n",
    "\n",
    "The `DETR` (Detection Transformer) class is designed for object detection tasks. It loads a pre-trained DETR model for detecting objects in an image, applies necessary transformations to the input, rescales bounding boxes, and provides a visualization method for plotting detected objects on the image.\n",
    "\n",
    "### Attributes:\n",
    "- **CLASSES** : *list of str*  \n",
    "    A list of object classes the model can detect. Each index corresponds to a specific class.\n",
    "  \n",
    "- **COLORS** : *list of list of float*  \n",
    "    A list of RGB color triplets used for visualizing detected bounding boxes. The colors are cycled for multiple detections.\n",
    "\n",
    "- **transform** : *torchvision.transforms.Compose*  \n",
    "    A composition of image transformations applied to the input image before feeding it into the model.  \n",
    "    This includes converting the image to a tensor and normalizing it.\n",
    "\n",
    "- **model** : *DetrForObjectDetection*  \n",
    "    The pre-trained DETR model loaded using Hugging Face's `from_pretrained` method, used for detecting objects in the input image.\n",
    "\n",
    "### Methods:\n",
    "- **`__init__()`**  \n",
    "    Initializes the DETR class by setting up object classes, colors for visualization, image transformations, and loading the pre-trained DETR model.  \n",
    "    The model is moved to the appropriate device (CPU/GPU) and set to evaluation mode.\n",
    "  \n",
    "- **`box_cxcywh_to_xyxy(x)`**  \n",
    "    Converts bounding boxes from the center (cx, cy) and width/height (cxcywh) format to top-left (x1, y1) and bottom-right (x2, y2) corner format (xyxy).\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **x** : *torch.Tensor*  \n",
    "            Bounding boxes in (center_x, center_y, width, height) format.\n",
    "    \n",
    "    - **Returns**:\n",
    "        - **torch.Tensor**  \n",
    "            Bounding boxes in (x_min, y_min, x_max, y_max) format.\n",
    "\n",
    "- **`rescale_bboxes(out_bbox, size)`**  \n",
    "    Rescales predicted bounding boxes to match the original size of the input image.\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **out_bbox** : *torch.Tensor*  \n",
    "            Bounding boxes predicted by the model, in relative format (0 to 1).\n",
    "        - **size** : *tuple*  \n",
    "            Original width and height of the image.\n",
    "    \n",
    "    - **Returns**:\n",
    "        - **torch.Tensor**  \n",
    "            Rescaled bounding boxes in absolute pixel coordinates.\n",
    "\n",
    "- **`detect(im)`**  \n",
    "    Performs object detection on the input image, returning class probabilities and bounding boxes for detected objects.\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **im** : *PIL.Image*  \n",
    "            Input image for object detection.\n",
    "    \n",
    "    - **Returns**:\n",
    "        - **Tuple[torch.Tensor, torch.Tensor]**  \n",
    "            - `probas`: Class probabilities for detected objects.\n",
    "            - `bboxes_scaled`: Rescaled bounding boxes for detected objects.\n",
    "\n",
    "- **`visualize(im, probas, bboxes)`**  \n",
    "    Visualizes detected bounding boxes and class probabilities on the input image, using `matplotlib` to draw boxes and labels.\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **im** : *PIL.Image*  \n",
    "            Original input image.\n",
    "        - **probas** : *torch.Tensor*  \n",
    "            Predicted class probabilities for detected objects.\n",
    "        - **bboxes** : *torch.Tensor*  \n",
    "            Bounding boxes for detected objects, scaled to the image size.\n",
    "\n",
    "### Summary:\n",
    "- `__init__`: Initializes the DETR model by loading the pre-trained weights.\n",
    "- `detect`: Takes an input image and performs object detection, returning the bounding boxes and class predictions for detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fe617",
   "metadata": {
    "id": "9b9fe617"
   },
   "outputs": [],
   "source": [
    "class DETR:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.CLASSES = [\n",
    "            'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "            'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "            'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "            'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "            'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "            'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "            'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "            'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "            'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "            'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush'\n",
    "        ]\n",
    "\n",
    "        self.COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098],\n",
    "                       [0.929, 0.694, 0.125], [0, 0, 1], [0.466, 0.674, 0.188],\n",
    "                       [0.301, 0.745, 0.933]]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.model = DetrForObjectDetection.from_pretrained(CONFIG['detr_model_path'], revision=\"no_timm\")\n",
    "        self.model.to(CONFIG['device'])\n",
    "        self.model.eval()\n",
    "\n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "        x_c, y_c, w, h = x.unbind(1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "        return torch.stack(b, dim=1).to(CONFIG['device'])\n",
    "\n",
    "    def rescale_bboxes(self, out_bbox, size):\n",
    "        img_w, img_h = size\n",
    "        b = self.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).to(CONFIG['device'])\n",
    "        return b\n",
    "\n",
    "\n",
    "    def detect(self, im):\n",
    "        img = self.transform(im).unsqueeze(0).to(CONFIG['device'])\n",
    "        assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'Image too large'\n",
    "        outputs = self.model(img)\n",
    "        probas = outputs['logits'].softmax(-1)[0, :, :-1]\n",
    "        keep = probas.max(-1).values > 0.7\n",
    "        bboxes_scaled = self.rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "        return probas[keep], bboxes_scaled\n",
    "\n",
    "    def visualize(self, im, probas, bboxes):\n",
    "        \"\"\"\n",
    "        Visualizes the detected bounding boxes and class probabilities on the image.\n",
    "\n",
    "        Parameters:\n",
    "            im (PIL.Image): The original input image.\n",
    "            probas (Tensor): Class probabilities for detected objects.\n",
    "            bboxes (Tensor): Bounding boxes for detected objects.\n",
    "        \"\"\"\n",
    "        # Convert image to RGB format for matplotlib\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(im)\n",
    "        ax = plt.gca()\n",
    "\n",
    "        # Iterate over detections and draw bounding boxes and labels\n",
    "        for p, (xmin, ymin, xmax, ymax), color in zip(probas, bboxes, self.COLORS * 100):\n",
    "            # Detach tensors and convert to float\n",
    "            xmin, ymin, xmax, ymax = map(lambda x: x.detach().cpu().numpy().item(), (xmin, ymin, xmax, ymax))\n",
    "\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                        fill=False, color=color, linewidth=3))\n",
    "            cl = p.argmax()\n",
    "            text = f'{self.CLASSES[cl]}: {p[cl].detach().cpu().numpy():0.2f}'  # Detach probability as well\n",
    "            ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f700e",
   "metadata": {
    "id": "0c9f700e"
   },
   "source": [
    "# GLPDepth Class for Monocular Depth Estimation\n",
    "\n",
    "The `GLPDepth` class is designed to perform monocular depth estimation from a single input image. It leverages a pre-trained GLPDepth model to extract features and predict the depth map of the image. The model runs inference on the appropriate hardware (CPU or GPU) and operates in evaluation mode.\n",
    "\n",
    "### Attributes:\n",
    "- **`feature_extractor`** : *GLPNFeatureExtractor*  \n",
    "    A pre-trained feature extractor that processes the input image, converting it into a tensor format suitable for the depth estimation model.\n",
    "  \n",
    "- **`model`** : *GLPNForDepthEstimation*  \n",
    "    A pre-trained GLPDepth model used to predict the depth map from the input image. The model is moved to the appropriate device (CPU or GPU) and set to evaluation mode for inference.\n",
    "\n",
    "### Methods:\n",
    "- **`__init__()`**  \n",
    "    Initializes the `GLPDepth` class by loading the pre-trained feature extractor and depth estimation model.  \n",
    "    The model is automatically moved to the correct device and configured for evaluation.\n",
    "\n",
    "- **`predict(img, img_shape)`**  \n",
    "    Predicts the depth map for the input image using the GLPDepth model.\n",
    "    \n",
    "    - **Parameters**:\n",
    "        - **img** : *PIL.Image*  \n",
    "            The input image for which the depth map will be estimated.\n",
    "        \n",
    "        - **img_shape** : *tuple*  \n",
    "            The original dimensions of the input image, represented as (height, width).\n",
    "    \n",
    "    - **Returns**:\n",
    "        - **np.ndarray**  \n",
    "            The predicted depth map as a NumPy array, resized to match the original image dimensions.\n",
    "\n",
    "### Summary:\n",
    "- `__init__`: Initializes the GLPN depth estimation model by loading the pre-trained weights.\n",
    "- `predict`: Takes an input image and returns the predicted depth map, which can be used to estimate the distance of objects in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b42685",
   "metadata": {
    "id": "22b42685"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Apr  9 04:08:02 2022\n",
    "@author: Admin_with ODD Team\n",
    "\n",
    "Edited by our team : Sat Oct 5 10:00 2024\n",
    "\n",
    "references: https://github.com/vinvino02/GLPDepth\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n",
    "\n",
    "# GLPDepth Model Class\n",
    "class GLPDepth:\n",
    "    def __init__(self):\n",
    "        # Load feature extractor and model from pretrained path\n",
    "        self.feature_extractor = GLPNFeatureExtractor.from_pretrained(CONFIG['glpn_model_path'])\n",
    "        self.model = GLPNForDepthEstimation.from_pretrained(CONFIG['glpn_model_path'])\n",
    "\n",
    "        # Move model to the right device (GPU or CPU)\n",
    "        self.model.to(CONFIG['device'])\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, img: Image.Image, img_shape: tuple):\n",
    "        \"\"\"Predict the depth map of the input image.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): Input image for depth estimation.\n",
    "            img_shape (tuple): Original image size (height, width).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted depth map in numpy array format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Preprocess image and move to the appropriate device\n",
    "            pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values.to(CONFIG['device'])\n",
    "\n",
    "            # Get model output\n",
    "            outputs = self.model(pixel_values)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "\n",
    "            # Resize depth prediction to original image size\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                predicted_depth.unsqueeze(1),\n",
    "                size=img_shape[:2],  # Interpolate to original image size (H, W)\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            prediction = prediction.squeeze().cpu().numpy()  # Convert to numpy array (shape: (H, W))\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def plot_depth_map(self, depth_map: np.ndarray, cmap='plasma'):\n",
    "        \"\"\"Plot the predicted depth map using matplotlib.\n",
    "\n",
    "        Args:\n",
    "            depth_map (np.ndarray): The predicted depth map (H, W).\n",
    "            cmap (str): The colormap for visualization.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(depth_map, cmap=cmap)\n",
    "        cbar = plt.colorbar(label='Depth Value', orientation='horizontal')\n",
    "        cbar.ax.tick_params(labelsize=12)  # Optional: set the size of the ticks\n",
    "        plt.axis('off')  # Turn off the axis\n",
    "\n",
    "        # Adjust layout to avoid overlap\n",
    "        plt.subplots_adjust(bottom=0.15)  # Adjust the bottom to fit the colorbar\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ee721",
   "metadata": {
    "id": "437ee721"
   },
   "source": [
    "# Utility Function for Predicting Z-location for a Single Row\n",
    "\n",
    "This section defines a utility function that processes a single row of data, containing bounding box coordinates, depth information, and object class type. The function predicts the Z-location of the object using the LSTM model.\n",
    "\n",
    "### Parameters:\n",
    "- `row`: A single row of data that contains information about the detected object, including bounding box and depth information.\n",
    "- `ZlocE`: The pre-loaded LSTM model used to predict the Z-location.\n",
    "- `scaler`: The scaler used for normalizing the input data before making predictions.\n",
    "\n",
    "### Returns:\n",
    "- The predicted Z-location of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e56be7",
   "metadata": {
    "id": "93e56be7",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.176545Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_z_location_single_row(row, ZlocE, scaler):\n",
    "\n",
    "    # One-hot encoding of class type\n",
    "    class_type = row['class']\n",
    "\n",
    "    if class_type == 'bicycle':\n",
    "        class_tensor = torch.tensor([[0, 1, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "    elif class_type == 'car':\n",
    "        class_tensor = torch.tensor([[0, 0, 1, 0, 0, 0]], dtype=torch.float32)\n",
    "    elif class_type == 'person':\n",
    "        class_tensor = torch.tensor([[0, 0, 0, 1, 0, 0]], dtype=torch.float32)\n",
    "    elif class_type == 'train':\n",
    "        class_tensor = torch.tensor([[0, 0, 0, 0, 1, 0]], dtype=torch.float32)\n",
    "    elif class_type == 'truck':\n",
    "        class_tensor = torch.tensor([[0, 0, 0, 0, 0, 1]], dtype=torch.float32)\n",
    "    else :\n",
    "        class_tensor = torch.tensor([[1, 0, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "    # Prepare input data (bounding box + depth info)\n",
    "    input_data = np.array([row[['xmin', 'ymin', 'xmax', 'ymax', 'width', 'height', 'depth_mean', 'depth_median', 'depth_mean_trim']].values], dtype=np.float32)\n",
    "    input_data = torch.from_numpy(input_data)\n",
    "\n",
    "    # Concatenate class information\n",
    "    input_data = torch.cat([input_data, class_tensor], dim=-1)\n",
    "\n",
    "    # Scale the input data\n",
    "    scaled_input = torch.tensor(scaler.transform(input_data), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Use the LSTM model to predict the Z-location\n",
    "    z_loc_prediction = ZlocE.predict(scaled_input).detach().numpy()[0]\n",
    "\n",
    "    return z_loc_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4269a",
   "metadata": {
    "id": "81e4269a"
   },
   "source": [
    "# Class for Processing Object Detections and Handling Overlaps\n",
    "\n",
    "This section defines a class for processing object detection results, managing overlapping bounding boxes, and computing depth statistics for detected objects. This is useful for refining the results and eliminating unnecessary or redundant bounding boxes.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "- **`process_detections(scores, boxes, depth_map, detr)`**  \n",
    "    Processes the detected bounding boxes and computes depth statistics for each detection. It also assigns object classes and handles class filtering, RGB color assignment, and depth calculations.\n",
    "    \n",
    "    - **Args**:\n",
    "        - `scores` (*list*): Class prediction scores from the object detection model.\n",
    "        - `boxes` (*numpy.ndarray*): Bounding boxes in the format \\[`xmin`, `ymin`, `xmax`, `ymax`\\].\n",
    "        - `depth_map` (*numpy.ndarray*): Depth map corresponding to the image.\n",
    "        - `detr` (*object*): Pretrained object detection model (e.g., DETR), used for extracting object class information.\n",
    "    \n",
    "    - **Returns**:\n",
    "        - `pandas.DataFrame`: A DataFrame containing processed detection information for each bounding box, including:\n",
    "            - Bounding box coordinates (`xmin`, `ymin`, `xmax`, `ymax`)\n",
    "            - Depth statistics (`mean`, `trimmed mean`, `median`)\n",
    "            - Object class\n",
    "            - RGB color values associated with the class.\n",
    "\n",
    "- **`handle_overlaps(depth_map)`**  \n",
    "    Detects and processes overlapping bounding boxes. If two objects overlap by more than 70%, the farther object is removed, and depth statistics are recalculated for the overlapping area. Depth values within the overlapping region are also adjusted accordingly.\n",
    "    \n",
    "    - **Args**:\n",
    "        - `depth_map` (*numpy.ndarray*): Depth map corresponding to the image, used for recalculating depth in overlapping regions.\n",
    "\n",
    "### Summary:\n",
    "- `__init__`: Initializes the class with any required attributes, such as thresholds for overlap.\n",
    "- `process_detections`: Processes the detected objects by computing depth-related statistics and handling overlaps between bounding boxes. The final output consists of refined object detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d156f",
   "metadata": {
    "id": "aa2d156f",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.179851Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PROCESSING :\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process_detections(self, scores, boxes, depth_map, detr):\n",
    "        \"\"\"\n",
    "        Processes object detections, computes depth statistics, and handles overlapping bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            scores (list): List of class prediction scores from the object detection model.\n",
    "            boxes (numpy.ndarray): Bounding boxes in the format [xmin, ymin, xmax, ymax].\n",
    "            depth_map (numpy.ndarray): Depth map corresponding to the image.\n",
    "            detr (object): Pretrained object detection model (e.g., detr) containing class information.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: Processed dataset containing bounding box coordinates,\n",
    "                            depth statistics, and object class.\n",
    "        \"\"\"\n",
    "        # Initialize a DataFrame for storing results\n",
    "        self.data = pd.DataFrame(columns=['xmin','ymin','xmax','ymax','width', 'height','depth_mean_trim','depth_mean','depth_median', 'class', 'rgb'])\n",
    "\n",
    "        # Iterate over detected bounding boxes and their corresponding scores\n",
    "        for p, (xmin, ymin, xmax, ymax) in zip(scores, boxes.tolist()):\n",
    "            # Identify the class with the highest score\n",
    "            detected_class = p.argmax()\n",
    "            class_label = detr.CLASSES[detected_class]\n",
    "\n",
    "            # Filter for relevant object classes\n",
    "            if class_label == 'motorcycle':\n",
    "                class_label = 'bicycle'\n",
    "            elif class_label == 'bus':\n",
    "                class_label = 'train'\n",
    "            elif class_label not in ['person', 'truck', 'car', 'bicycle', 'train']:\n",
    "                class_label = 'Misc'\n",
    "\n",
    "            if class_label in ['Misc', 'person', 'truck', 'car', 'bicycle', 'train']:\n",
    "                # Assign RGB color for the detected class\n",
    "                class_index = ['Misc', 'person', 'truck', 'car', 'bicycle', 'train'].index(class_label)\n",
    "                r, g, b = detr.COLORS[class_index]\n",
    "                rgb = (r * 255, g * 255, b * 255)\n",
    "\n",
    "                # Calculate bounding box dimensions\n",
    "                width, height = xmax - xmin, ymax - ymin\n",
    "                xmin, ymin = max(0, int(xmin)), max(0, int(ymin))\n",
    "\n",
    "                # Compute depth statistics within the bounding box\n",
    "                bbox_depth = depth_map[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "                depth_mean = bbox_depth.mean()\n",
    "                depth_median = np.median(bbox_depth)\n",
    "                depth_trimmed_mean = stats.trim_mean(bbox_depth.flatten(), 0.2)\n",
    "                #depth_max = bbox_depth.max()\n",
    "\n",
    "                # Store the calculated data in the DataFrame\n",
    "                new_row = pd.DataFrame([[xmin, ymin, xmax, ymax, width, height, depth_trimmed_mean ,depth_mean, depth_median, class_label, rgb]],\n",
    "                                    columns=self.data.columns)\n",
    "                self.data = pd.concat([self.data, new_row], ignore_index=True)\n",
    "\n",
    "        # Handle overlapping bounding boxes\n",
    "        self.handle_overlaps(depth_map)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "\n",
    "    def handle_overlaps(self, depth_map):\n",
    "        \"\"\"\n",
    "        Handles overlapping bounding boxes by removing the farther object\n",
    "        or recalculating depth statistics for the overlapping region.\n",
    "\n",
    "        Args:\n",
    "            depth_map (numpy.ndarray): Depth map corresponding to the image.\n",
    "        \"\"\"\n",
    "        # Reset the index for easy iteration\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Lists to track the bounding box coordinates\n",
    "        xmin_list, ymin_list, xmax_list, ymax_list = [], [], [], []\n",
    "\n",
    "        # Loop through each bounding box in the dataset\n",
    "        for index, (xmin, ymin, xmax, ymax) in self.data[['xmin', 'ymin', 'xmax', 'ymax']].iterrows():\n",
    "            xmin_list.insert(0, xmin)\n",
    "            ymin_list.insert(0, ymin)\n",
    "            xmax_list.insert(0, xmax)\n",
    "            ymax_list.insert(0, ymax)\n",
    "\n",
    "            # Compare the current bounding box with all previous ones\n",
    "            for i in range(len(xmin_list) - 1):\n",
    "                # Check Y-axis overlap\n",
    "                y_range1 = np.arange(int(ymin_list[0]), int(ymax_list[0]) + 1)\n",
    "                y_range2 = np.arange(int(ymin_list[i + 1]), int(ymax_list[i + 1]) + 1)\n",
    "                y_intersection = np.intersect1d(y_range1, y_range2)\n",
    "\n",
    "                if len(y_intersection) >= 1:\n",
    "                    # Check X-axis overlap\n",
    "                    x_range1 = np.arange(int(xmin_list[0]), int(xmax_list[0]) + 1)\n",
    "                    x_range2 = np.arange(int(xmin_list[i + 1]), int(xmax_list[i + 1]) + 1)\n",
    "                    x_intersection = np.intersect1d(x_range1, x_range2)\n",
    "\n",
    "                    if len(x_intersection) >= 1:\n",
    "                        # Calculate the areas of the bounding boxes and their intersection\n",
    "                        area1 = (y_range1.max() - y_range1.min()) * (x_range1.max() - x_range1.min())\n",
    "                        area2 = (y_range2.max() - y_range2.min()) * (x_range2.max() - x_range2.min())\n",
    "                        area_intersection = (y_intersection.max() - y_intersection.min()) * (x_intersection.max() - x_intersection.min())\n",
    "\n",
    "                        # If more than 70% overlap, remove the farther object\n",
    "                        if area_intersection / area1 >= 0.70 or area_intersection / area2 >= 0.70:\n",
    "                            if area1 < area2:\n",
    "                                self.data.drop(index=index, inplace=True)\n",
    "                            else:\n",
    "                                self.data.drop(index=index - (i + 1), inplace=True)\n",
    "\n",
    "                        # If partial overlap, recalculate depth for the overlapping region\n",
    "                        elif area_intersection / area1 > 0 or area_intersection / area2 > 0:\n",
    "                            # Convert to integers for indexing\n",
    "                            y_min_idx = int(y_intersection.min())\n",
    "                            y_max_idx = int(y_intersection.max())\n",
    "                            x_min_idx = int(x_intersection.min())\n",
    "                            x_max_idx = int(x_intersection.max())\n",
    "\n",
    "                            if area1 < area2:\n",
    "                                # Check bounds before slicing\n",
    "                                if (0 <= y_min_idx < depth_map.shape[0]) and (0 <= y_max_idx < depth_map.shape[0]) and \\\n",
    "                                (0 <= x_min_idx < depth_map.shape[1]) and (0 <= x_max_idx < depth_map.shape[1]):\n",
    "                                    depth_map[y_min_idx:y_max_idx, x_min_idx:x_max_idx] = np.nan\n",
    "                                    bbox_depth = depth_map[int(ymin_list[0]):int(ymax_list[0]), int(xmin_list[0]):int(xmax_list[0])]\n",
    "                                    self.data.at[index, 'depth_mean'] = np.nanmean(bbox_depth)\n",
    "                                else:\n",
    "                                    print(\"Index out of bounds for depth map:\", y_min_idx, y_max_idx, x_min_idx, x_max_idx)\n",
    "                            else:\n",
    "                                # Similar bounds checking for the other box\n",
    "                                if (0 <= y_min_idx < depth_map.shape[0]) and (0 <= y_max_idx < depth_map.shape[0]) and \\\n",
    "                                (0 <= x_min_idx < depth_map.shape[1]) and (0 <= x_max_idx < depth_map.shape[1]):\n",
    "                                    depth_map[y_min_idx:y_max_idx, x_min_idx:x_max_idx] = np.nan\n",
    "                                    bbox_depth = depth_map[int(ymin_list[i + 1]):int(ymax_list[i + 1]), int(xmin_list[i + 1]):int(xmax_list[i + 1])]\n",
    "                                    self.data.at[index - (i + 1), 'depth_mean'] = np.nanmean(bbox_depth)\n",
    "                                else:\n",
    "                                    print(\"Index out of bounds for depth map:\", y_min_idx, y_max_idx, x_min_idx, x_max_idx)\n",
    "\n",
    "        # Reset index after removing rows\n",
    "        self.data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d3804",
   "metadata": {
    "id": "154d3804"
   },
   "source": [
    "# Function for Generating JSON Output\n",
    "\n",
    "This function takes the processed data, predicts the Z-location for each detected object, and returns the results in JSON format. The JSON structure will include the bounding box, object class, and the predicted Z-location for each object.\n",
    "\n",
    "### Parameters:\n",
    "- `data`: The data containing the bounding box coordinates, depth information, and object class type.\n",
    "- `ZlocE`: The pre-loaded LSTM model for Z-location prediction.\n",
    "- `scaler`: The scaler used for normalizing the input data.\n",
    "\n",
    "### Returns:\n",
    "- A JSON structure with the predicted Z-locations and additional details for each detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d8001",
   "metadata": {
    "id": "461d8001"
   },
   "outputs": [],
   "source": [
    "def generate_output_json(data, ZlocE, scaler):\n",
    "\n",
    "    output_json = []\n",
    "\n",
    "    # Iterate over each row in the data\n",
    "    for i, row in data.iterrows():\n",
    "        # Predict distance for each object using the single-row prediction function\n",
    "        distance = predict_z_location_single_row(row, ZlocE, scaler)\n",
    "\n",
    "        # Create object info dictionary\n",
    "        object_info = {\n",
    "            \"class\": row[\"class\"],  # Object class (e.g., 'car', 'truck')\n",
    "            \"distance_estimated\": float(distance),  # Convert distance to float (if necessary)\n",
    "            \"features\": {\n",
    "                \"xmin\": float(row[\"xmin\"]),  # Bounding box xmin\n",
    "                \"ymin\": float(row[\"ymin\"]),  # Bounding box ymin\n",
    "                \"xmax\": float(row[\"xmax\"]),  # Bounding box xmax\n",
    "                \"ymax\": float(row[\"ymax\"]),  # Bounding box ymax\n",
    "                \"mean_depth\": float(row[\"depth_mean\"]),  # Depth mean\n",
    "                \"depth_mean_trim\": float(row[\"depth_mean_trim\"]),  # Depth mean trim\n",
    "                \"depth_median\": float(row[\"depth_median\"]),  # Depth median\n",
    "                \"width\": float(row[\"width\"]),  # Object width\n",
    "                \"height\": float(row[\"height\"])  # Object height\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Append each object info to the output JSON list\n",
    "        output_json.append(object_info)\n",
    "\n",
    "    # Return the final JSON output structure\n",
    "    return {\"objects\": output_json}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb143d0e",
   "metadata": {
    "id": "eb143d0e"
   },
   "source": [
    "# Initialize the FastAPI Application\n",
    "\n",
    "This section initializes the FastAPI application, which will serve as the main entry point for interacting with the models. The API will handle WebSocket requests for uploading images and receiving object detection and depth estimation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17321fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c17321fb",
    "outputId": "c69d8d38-4b6d-4460-8722-0375180d8bf8"
   },
   "outputs": [],
   "source": [
    "'''app = FastAPI(title=\"WebSocket Image Upload API\", description=\"API for uploading images via WebSocket and receiving object detection and depth estimation results.\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0c6ae",
   "metadata": {
    "id": "20c0c6ae"
   },
   "source": [
    "# Load Models and Scaler Outside the WebSocket Route for Efficiency\n",
    "\n",
    "To improve the performance of the WebSocket route, the object detection (DETR), depth estimation (GLPN), and Z-location prediction (LSTM) models, as well as the scaler, are loaded outside the WebSocket route. This ensures that the models do not need to be reloaded with each incoming request, reducing latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f80506",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87f80506",
    "outputId": "f74b1d71-a212-4535-ca41-62467a4a6494"
   },
   "outputs": [],
   "source": [
    "detr = DETR()\n",
    "glpn = GLPDepth()\n",
    "zlocE = LSTM_Model()\n",
    "scaler = pickle.load(open(CONFIG['lstm_scaler_path'], 'rb'))\n",
    "processing = PROCESSING()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91497dd0",
   "metadata": {
    "id": "91497dd0"
   },
   "source": [
    "# Define Route for Serving HTML Documentation\n",
    "\n",
    "This route serves HTML documentation for the API. It responds to GET requests at the root (`\"/\"`) and returns an HTML response that describes how to interact with the FastAPI service. This can be useful for providing a user-friendly interface or API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf14055",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "4cf14055",
    "outputId": "526ab0ab-ba78-430f-a84c-3f9460edd17b",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.190566Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Serve the HTML documentation\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def get_docs():\n",
    "\n",
    "    # Get the directory of the current script\n",
    "    html_path = os.path.join(os.path.dirname(__file__), \"docs.html\")\n",
    "\n",
    "    if not os.path.exists(html_path):\n",
    "        return HTMLResponse(content=\"docs.html file not found\", status_code=404)\n",
    "\n",
    "    with open(html_path, \"r\") as f:\n",
    "        return HTMLResponse(f.read())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98ae4d",
   "metadata": {
    "id": "3b98ae4d"
   },
   "source": [
    "# Define Asynchronous Functions for Running DETR Detection and GLPN Depth estimation\n",
    "\n",
    "Two asynchronous functions for running object detection using the DETR model and performing depth estimation using the GLPN model. The functions will be executed in a separate threads to avoid blocking the main application, ensuring better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618f9c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "9618f9c1",
    "outputId": "0793c4cb-276c-4308-8a33-1da5309e6e72"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Run DETR detection in a separate thread\n",
    "async def run_detr(pil_image):\n",
    "    return await asyncio.to_thread(detr.detect, pil_image)\n",
    "\n",
    "# Run GLPN depth estimation in a separate thread\n",
    "async def run_glpn(pil_image, img_shape):\n",
    "    return await asyncio.to_thread(glpn.predict, pil_image, img_shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f371acc",
   "metadata": {
    "id": "1f371acc"
   },
   "source": [
    "# Define WebSocket Endpoint for Handling Image Uploads and Processing Requests\n",
    "\n",
    "This WebSocket endpoint handles image uploads from clients. When an image is uploaded, it runs the necessary object detection and depth estimation models, and then returns the results to the client.\n",
    "\n",
    "### Parameters:\n",
    "- `websocket`: The WebSocket connection object, which is used to receive images from the client and send the results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fdc3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "205fdc3a",
    "outputId": "f76fbce2-2338-4483-c66a-9ff9156a3b00"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@app.websocket(\"/ws/predict\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    while True:\n",
    "        try:\n",
    "            # Receive raw bytes (image data)\n",
    "            image_bytes = await websocket.receive_bytes()\n",
    "            # Convert bytes to a NumPy array\n",
    "            nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "            # Decode the image\n",
    "            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "            # Process the frame\n",
    "            # frame = cv2.resize(frame, (1280, 640))\n",
    "            color_converted = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(color_converted)\n",
    "            img_shape = color_converted.shape[0:2]  # (height, width)\n",
    "\n",
    "            # DETR Object Detection\n",
    "            # scores, boxes = detr.detect(pil_image)\n",
    "\n",
    "            # GLPN Depth Estimation\n",
    "            # depth_map = glpn.predict(pil_image, img_shape)\n",
    "\n",
    "            # Run DETR and GLPN in parallel\n",
    "            detr_result, depth_map = await asyncio.gather(\n",
    "                run_detr(pil_image),\n",
    "                run_glpn(pil_image, img_shape)\n",
    "            )\n",
    "\n",
    "            # Unpack the DETR detection results\n",
    "            scores, boxes = detr_result\n",
    "\n",
    "            # Process bounding boxes and overlap them with the depth map\n",
    "            pdata = processing.process_detections(scores, boxes, depth_map, detr)\n",
    "\n",
    "            # Generate the output JSON\n",
    "            output_json = generate_output_json(pdata, zlocE, scaler)\n",
    "\n",
    "            # Send the output back to the client\n",
    "            await websocket.send_text(json.dumps(output_json))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            await websocket.send_text(f\"Error processing image: {str(e)}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WkLWvSS0-1lj",
   "metadata": {
    "id": "WkLWvSS0-1lj"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "sample_image = cv2.imread(\"sample_image.png\")\n",
    "\n",
    "sample_image = cv2.resize(sample_image, (1280, 640))\n",
    "color_converted = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "pil_image = Image.fromarray(color_converted)\n",
    "img_shape = color_converted.shape[0:2]  # (height, width)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "FNmWLCKU-_Xm",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xLx8I0B_rX7",
   "metadata": {
    "id": "7xLx8I0B_rX7"
   },
   "outputs": [],
   "source": [
    "# DETR Object Detection\n",
    "scores, boxes = detr.detect(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5swnzOZQC9MA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5swnzOZQC9MA",
    "outputId": "964360be-ad64-4119-e31b-c7094314ff79"
   },
   "outputs": [],
   "source": [
    "# Show Object Detection\n",
    "detr.visualize(pil_image, scores, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "InxhFLZb_ugY",
   "metadata": {
    "id": "InxhFLZb_ugY",
    "ExecuteTime": {
     "start_time": "2025-03-21T15:29:27.204189Z"
    }
   },
   "outputs": [],
   "source": [
    "# GLPN Depth Estimation\n",
    "depth_map = glpn.predict(pil_image, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3zS6OZEdMEZk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "3zS6OZEdMEZk",
    "outputId": "21b12a27-b69d-44d0-df45-2afaa783bcd2"
   },
   "outputs": [],
   "source": [
    "# Show Depth Map\n",
    "glpn.plot_depth_map(depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uyAIWWt2_v7j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "uyAIWWt2_v7j",
    "outputId": "f52d14a3-cf1b-40df-b967-08698a6d3cc9"
   },
   "outputs": [],
   "source": [
    "# Process bounding boxes and overlap them with the depth map\n",
    "pdata = processing.process_detections(scores, boxes, depth_map, detr)\n",
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5hbbIm8_xQd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5hbbIm8_xQd",
    "outputId": "a97a89a4-bcb8-4dc4-f1cb-38e9762910ef"
   },
   "outputs": [],
   "source": [
    "# Generate the output JSON\n",
    "output_json = generate_output_json(pdata, zlocE, scaler)\n",
    "output_json"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
